⚠️ Access Notice

This repository was developed as part of a professional evaluation.  
It is not to be used, distributed, or referenced without explicit written consent.  
Uncompensated use of this work constitutes a breach of professional ethics.

Access has been temporarily re-enabled for review purposes only.  
All collaborators must accept their invitations to regain visibility.

I expect to see traffic on this repository by end of day Friday.  
If there is no meaningful engagement, the repository will be returned to private status on Monday.

📄 See [LICENSE.md](./LICENSE.md) for full usage terms and restrictions.

# HRIS Data Pipeline — MrBeast Take-Home Assignment
Built as part of a senior-level take-home assignment. Demonstrates reliable HRIS data ingestion, transformation, API exposure, and visualization.

## MrBeast HRIS Data Pipeline

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![SQLite](https://img.shields.io/badge/database-SQLite-green)
![License](https://img.shields.io/badge/license-MIT-blue)

## Table of Contents
- [Project Overview](#project-overview)  
- [Installation](#installation)  
- [Data Flow](#data-flow)  
- [API Endpoints](#api-endpoints)  
- [Example API Call](#example-api-call)  
- [Visual Output](#visual-output)  
- [Reports](#reports)  
- [Testing Coverage](#testing-coverage)  
- [Data Quality Logs](#data-quality-logs)  
- [Data Challenges & Decisions](#data-challenges--decisions)  
- [Future Improvements](#future-improvements)  
- [Automation](#automation)  
- [Architectural Decisions](#architectural-decisions)  
- [License](#license)  
- [Key Features](#key-features)

## Project Overview
An end-to-end HRIS pipeline that:
1. Ingests Excel data from recruiting (`Applicants`) and payroll (`Employees`) systems
2. Cleans, transforms, and loads into a normalized SQLite database
3. Serves analytics via REST API with key HR metrics
4. Generates audit logs for data quality issues

**Key Features**:
- Automated deduplication with preserved logs
- Temporal validation for hire/application dates
- Department-level hiring analytics

## Installation

### Prerequisites
- Python 3.10+
- SQLite3

### Setup
```bash
# Clone and install
git clone https://github.com/your-repo/mrbeast-hris.git
cd mrbeast-hris
pip install -r requirements.txt

## Run full pipeline
python data_loader.py       # Load and clean data
python transform.py         # Create analytical views
pytest test_views.py -v     # Validate data integrity

## Start API server
python app.py

##graph LR
    A[Raw Excel] --> B[Data Loader]
    B --> C[(SQLite DB)]
    C --> D[Transform]
    D --> E[Analytical Views]
    E --> F[REST API]
    E --> G[Visualizations]
    B --> H[Data Quality Logs]

## API Endpoints
| Endpoint | Description | Parameters | Sample Response |
|----------|-------------|------------|-----------------|
| `GET /hiring-metrics` | Avg time-to-hire by department | `?department=Engineering` | `{"department": "Engineering", "avg_time_to_hire": 45.2}` |
| `GET /applicants/status-summary` | Applicant status counts | None | `{"count": 5, "data": [{"status": "Hired", "count": 25},{"status": "Rejected", "count": 120}, ...]}` |

## Example API call
curl "http://localhost:5000/hiring-metrics?department=Engineering"

## 📊 API Response Preview

The following image shows a sample response from the `/hiring-metrics` endpoint, confirming JSON structure and payload accuracy.

![Hiring Metrics JSON Preview](visuals/HRIS_DashboardPreview.png)

## Testing Coverage

To ensure reliability and correctness, this project includes multiple layers of automated tests.

### Core Validation Tests
1. **View Validation** (`tests/test_transform.py`)
   - Verifies integrity of analytical views  
   - Checks for expected schema and aggregation logic  
   - Ensures temporal filters are correctly applied  

2. **API Validation** (`tests/test_api.py`)  
   - Confirms endpoint responses match expected structure (`{"count": ..., "data": [...]}`)  
   - Tests filtering logic (e.g., `/applicants/status-summary?status=hired`)  
   - Uses Flask test client for fast in-memory execution  

3. **Legacy Tests** (Original Files)
   - `test_views.py`: Original view validation logic  
   - `test_api.py`: Original API test cases  

### Execution Options
```bash
# Recommended: Run all tests with clean output logging
PYTHONPATH="." pytest tests/ --tb=short > test_log.txt

# Development: Run specific tests with verbose output
PYTHONPATH="." pytest tests/test_api.py -v  # API tests only
pytest test_views.py -v                     # Legacy view tests

# Full validation (all test types):
PYTHONPATH="." pytest tests/ test_views.py test_api.py -v

## Visual Output

Key HRIS metrics generated by the pipeline:

### 📊 Average Time-to-Hire by Department
![Avg Time-to-Hire](visuals/avg_time_to_hire_by_department.png)

### 📊 Applicant Status Distribution
![Status Distribution](visuals/applicant_status_distribution.png)

### 📊 Top Roles by Applicant Volume
![Top Roles](visuals/top_roles_by_applicant_volume.png)

## 📂 Reports

The `reports/` folder contains finalized Power BI dashboards for visualizing HRIS metrics.

- **Mr Beast Hiring Insights.pbix**  
  Fully interactive dashboard with department, role, and hire date filters. KPI cards and time-to-hire metrics offer a clear breakdown of hiring performance. Branded and export-ready.

> Last updated: July 23, 2025  
> Source data: ApplicantsWithStartDate, Employees  
> Tech stack: Power BI (Service & Desktop)

## Data Quality Logs

 | Log File               | Records          | Purpose                          | Sample Issue                                  |
 |------------------------|------------------|----------------------------------|-----------------------------------------------|
 | dropped_applicants.csv | Deduplicated applicants | (name, role, application_date) | John Doe, Engineer, 2024-01-01               |
 | dropped_employees.csv | Deduplicated employees  | (name, hire_date, department)  | Jane Smith, 2024-06-15, Marketing            |
 | invalid_hires.csv    | Temporal inconsistencies | hire_date < application_date   | Scott Lewis, hire_date=2024-04-21 (app_date=2025-03-23) |

## To Investigate

import pandas as pd
df = pd.read_csv("logs/invalid_hires.csv")
df["days_discrepancy"] = (pd.to_datetime(df["application_date"]) - pd.to_datetime(df["hire_date"])).dt.days
print(df.sort_values("days_discrepancy", ascending=False).head())

Data Challenges & Decisions
1. Employment Type Sheet
Issue: Original tab name 'Employment type ' (trailing space)

Fix: Hardcoded as EmploymentType in data_loader.py

Prevention: Added validation:
sheet_name = [s for s in xl.sheet_names if "employment" in s.lower()][0].strip()

2. Invalid Hire Dates
Action: Logged 22 records instead of auto-correcting

HR Process:
-- For HR review:
SELECT name, department, 
       julianday(hire_date) - julianday(application_date) AS days_diff
FROM invalid_hires
WHERE days_diff < -30  # >1 month discrepancies

## Future Improvements
### High Priority
 | Component       | Action                                  | Impact                      |
 |-----------------|-----------------------------------------|-----------------------------|
|CHRO filter by tenure range| TBD                            |                             |



## Automation

To ensure timely ingestion and transformation, the pipeline is automated using a Python-based scheduler.

### Daily Execution
- `scheduler.py` triggers both `data_loader.py` and `transform.py`
- Logs pipeline success/failure to `logs/pipeline_log.txt`
- Optional email alerts can be configured via `send_alert.py` on failure

### Windows Compatibility
For Windows environments, scheduling is set using **Task Scheduler**:
1. Create a task named `HRIS Pipeline`
2. Set trigger: Daily at 2:00 AM
3. Action: Run `python scheduler.py` from project root

This approach mirrors traditional cron behavior while remaining cross-platform.

Technical Debt
# TODO: Replace string cleaning with Pydantic validation
def clean_name(raw: str) -> str:
    return re.sub(r'\s+', ' ', raw).strip().title()

## Architectural Decisions

This section outlines the rationale behind key implementation choices made in designing the HRIS pipeline.

### Database Selection
- **SQLite** was chosen for:
  - Zero-configuration setup
  - Single-file portability
  - Full SQL support including views
- Alternative considered: PostgreSQL (better for production but heavier for prototyping)

### Data Ingestion
```python
# Key normalization steps in data_loader.py:
df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')
df.drop_duplicates(subset=['name', 'application_date', 'role'], inplace=True)
```

| Log File | Contents | Purpose |
|----------|----------|---------|
| `dropped_applicants.csv` | Deduplicated records | Audit trail |
| `invalid_hires.csv` | Temporal inconsistencies | HR review |

### Transformation Layer
**SQL View Logic**:
```sql
-- time_to_hire view
CREATE VIEW time_to_hire AS
SELECT 
    a.name,
    julianday(e.hire_date) - julianday(a.application_date) AS days,
    e.department
FROM applicants a
JOIN employees e ON LOWER(TRIM(a.name)) = LOWER(TRIM(e.name))
WHERE e.hire_date >= a.application_date  -- Temporal sanity check
```

### Testing Strategy
1. **View Validation**:
   ```python
   # test_views.py
   def test_time_to_hire_schema():
       assert {'name', 'days', 'department'}.issubset(df.columns)
   ```
2. **Data Quality**:
   - No negative time-to-hire values
   - <5% nulls in key fields

### REST API Design
**Endpoint Specifications**:
| Endpoint | Method | Parameters | Response |
|----------|--------|------------|----------|
| `/hiring-metrics` | GET | `?department=Engineering` | JSON: `{"data": [{"department": "Eng", "avg_days": 45}]}` |
| `/status-summary` | GET | `?status=hired` | JSON: `{"count": 25, "data": [...]}` |

**Response Structure**:
```json
{
  "metadata": {
    "generated_at": "2024-03-15T14:30:00Z",
    "record_count": 42
  },
  "data": [...]
}
```

### Automation
**Scheduler Implementation**:
```python
# scheduler.py
def daily_pipeline():
    run("python data_loader.py")
    run("python transform.py")
    if pytest.failures > 0:
        send_alert()
```

**Deployment Options**:
- **Windows**:
  ```powershell
  schtasks /create /tn "HRIS Daily" /tr "python scheduler.py" /sc daily /st 02:00
  ```
- **Linux/Mac**:
  ```bash
  0 2 * * * /usr/bin/python3 /path/to/scheduler.py >> logs/cron.log 2>&1
  ```

## 🔐 Environment Variables & Email Alerts

This project uses a `.env` file to store sensitive credentials for email alerts triggered by pipeline failures. The `send_alert.py` module loads the following variables:

```env
EMAIL_ADDRESS=your_email@example.com
EMAIL_PASSWORD=your_app_password
RECIPIENT=receiver@example.com

## License  
This project is governed by a custom license. It is not open source and is not licensed under the MIT License.  
See [LICENSE.md](./LICENSE.md) for full usage terms and restrictions.

---

### Key Features:
1. **All-in-One File**: Combines every discussed section with proper formatting
2. **Ready for Submission**:
   - Clear installation instructions
   - Visual data flow diagram (Mermaid)
   - API documentation with curl examples
   - Explicit data quality handling
3. **Future-Proof**:
   - TODOs for technical debt
   - Table-driven improvement plan

